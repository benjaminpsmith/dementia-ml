{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from functions import *\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device.type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.75                     # In percent\n",
    "test_size = 1 - train_size           # In percent, calculated dynamically from train_size\n",
    "batch_size = 16                      # Size of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6400, 128, 128, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty array to store the image arrays and class\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Define the folder paths containing the images\n",
    "folder_paths = ['Dataset/Non_Demented/', 'Dataset/Very_Mild_Demented/', 'Dataset/Mild_Demented/', 'Dataset/Moderate_Demented/']\n",
    "classes = [r'Non demented', r'Very mildly demented', r'mild demented', r'moderate demented']\n",
    "\n",
    "# Loop over the images to save them in the list\n",
    "for path in folder_paths:\n",
    "    c = folder_paths.index(path)\n",
    "    items = os.listdir(path)\n",
    "    for picture in items:\n",
    "        file_path = os.path.join(path, picture)\n",
    "        # Open the image and convert it to a NumPy array\n",
    "        img = Image.open(file_path)\n",
    "        array_representation = np.asarray(img)\n",
    "\n",
    "        # Append the NumPy array to the list\n",
    "        X.append(array_representation)\n",
    "        Y.append(c)\n",
    "\n",
    "# Convert the list of image arrays to a NumPy arrayF\n",
    "X = np.array(X)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # , Normalizer, StandardScaler, RobustScaler\n",
    "# Transpose to make each image a row\n",
    "\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Normalize each row (i.e., each flattened image)\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "print(X.max())\n",
    "\n",
    "# Reshape back to the original shape\n",
    "X = X.reshape(len(X), 128, 128, 1)\n",
    "\n",
    "# Dynamically calculate the number of classes in dataset\n",
    "num_classes = len(np.unique(Y))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Assuming you have a class named MyDataset for your dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]\n",
    "        label = self.Y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Define transformations, you can adjust these based on your needs\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "# Create an instance of your dataset\n",
    "dataset = MyDataset(X=X, Y=Y, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [int(train_size * len(dataset)), len(dataset) - int(train_size * len(dataset))]\n",
    "    )\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [int(train_size * len(train_dataset)), len(train_dataset) - int(train_size * len(train_dataset))]\n",
    "    )\n",
    "\n",
    "# Define DataLoader for training and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, evaluation_metrics):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        # resnet = models.resnet18(pretrained=True)\n",
    "        resnet = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "        # Change number of input channels\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Add a custom fully connected layer\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "        # Initialize history dict\n",
    "        self.evaluation_metrics = evaluation_metrics\n",
    "        self.history = {key: [] for key in evaluation_metrics.keys()}\n",
    "        self.history_validation = {key: [] for key in evaluation_metrics.keys()}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        '''\n",
    "        Returns the predicted labels in evaluation mode: y_pred, y_true\n",
    "        '''\n",
    "        state = False if self.training is False else True\n",
    "        if state: self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for i, (x_minibatch, y_true_batch) in enumerate(dataloader):\n",
    "                y_pred_batch = F.softmax(self(x_minibatch), dim=1)\n",
    "                y_true.extend(y_true_batch.tolist())\n",
    "                y_pred.extend(y_pred_batch.tolist())\n",
    "        if state: self.train()\n",
    "        return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used for training: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model18\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m scheduler \u001b[39m=\u001b[39m StepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m train_network(model18, train_loader, criterion, optimizer, \u001b[39m70\u001b[39;49m, scheduler, validation_loader, device, early_stopper)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model18, \u001b[39m'\u001b[39m\u001b[39mmodel_resnet_18.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/machine_learning/dementia-ml/functions.py:69\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs, learning_rate_scheduler, dataloader_val, device, early_stopper)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[39m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 69\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     70\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m \u001b[39m# Update learning rate after \u001b[39;00m\n",
      "File \u001b[0;32m~/.local/conda/envs/ewha_ml/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/conda/envs/ewha_ml/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Now define the metrics you want to monitor during training and save them in a dict (Important: All need to take y_pred, y_cls, y_true as input (This order!))\n",
    "def criterion_function(y_pred, y_cls, y_true):\n",
    "    return criterion(torch.tensor(y_pred), torch.tensor(y_true))\n",
    "def accuracy_function(y_pred, y_cls, y_true):\n",
    "    return accuracy_score(y_true, y_cls)\n",
    "def recall_function(y_pred, y_cls, y_true):\n",
    "    return recall_score(y_true, y_cls, average='macro')\n",
    "metrics = {'loss' : criterion_function, 'acc' : accuracy_function, 'macro recall' : recall_function}\n",
    "\n",
    "# Define Model\n",
    "model18 = CustomResNet18(num_classes=4, evaluation_metrics=metrics)\n",
    "\n",
    "# Now define the loss (criterion), optimizer, lr_scheduler, \n",
    "early_stopper = EarlyStopper(model18, patience=5, min_delta=0)\n",
    "optimizer = torch.optim.Adam(model18.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_network(model18, train_loader, criterion, optimizer, 70, scheduler, validation_loader, device, early_stopper)\n",
    "\n",
    "torch.save(model18, 'model_resnet_18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = model18.evaluate(test_loader)\n",
    "y_cls = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_true, y_cls))\n",
    "print(confusion_matrix(y_true, y_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, evaluation_metrics):\n",
    "        super(CustomResNet50, self).__init__()\n",
    "        # resnet = models.resnet50(pretrained=True)\n",
    "        resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "        # Change number of input channels\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Add a custom fully connected layer\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "        # Initialize history dict\n",
    "        self.evaluation_metrics = evaluation_metrics\n",
    "        self.history = {key: [] for key in evaluation_metrics.keys()}\n",
    "        self.history_validation = {key: [] for key in evaluation_metrics.keys()}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        '''\n",
    "        Returns the predicted labels in evaluation mode: y_pred, y_true\n",
    "        '''\n",
    "        state = False if self.training is False else True\n",
    "        if state: self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for i, (x_minibatch, y_true_batch) in enumerate(dataloader):\n",
    "                y_pred_batch = F.softmax(self(x_minibatch), dim=1)\n",
    "                y_true.extend(y_true_batch.tolist())\n",
    "                y_pred.extend(y_pred_batch.tolist())\n",
    "        if state: self.train()\n",
    "        return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/toby_linux/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:30<00:00, 3.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used for training: cpu\n",
      "Epoch [1/1]  loss: 1.2698, acc: 0.4635, macro recall: 0.2500\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Now define the metrics you want to monitor during training and save them in a dict (Important: All need to take y_pred, y_cls, y_true as input (This order!))\n",
    "def criterion_function(y_pred, y_cls, y_true):\n",
    "    return criterion(torch.tensor(y_pred), torch.tensor(y_true))\n",
    "def accuracy_function(y_pred, y_cls, y_true):\n",
    "    return accuracy_score(y_true, y_cls)\n",
    "def recall_function(y_pred, y_cls, y_true):\n",
    "    return recall_score(y_true, y_cls, average='macro')\n",
    "metrics = {'loss' : criterion_function, 'acc' : accuracy_function, 'macro recall' : recall_function}\n",
    "\n",
    "# Define Model\n",
    "model50 = CustomResNet50(num_classes=4, evaluation_metrics=metrics)\n",
    "\n",
    "# Now define the loss (criterion), optimizer, lr_scheduler, \n",
    "early_stopper = EarlyStopper(model50, patience=5, min_delta=0)\n",
    "optimizer = torch.optim.Adam(model50.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_network(model50, train_loader, criterion, optimizer, 70, scheduler, validation_loader, device, early_stopper)\n",
    "\n",
    "torch.save(model50, 'model_resnet_50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = model50.evaluate(test_loader)\n",
    "y_cls = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_true, y_cls))\n",
    "print(confusion_matrix(y_true, y_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGoogleNet(nn.Module):\n",
    "    def __init__(self, num_classes, evaluation_metrics):\n",
    "        super(CustomGoogleNet, self).__init__()\n",
    "        # googlenet = models.googlenet(pretrained=True)\n",
    "        googlenet = models.googlenet(weights='GoogLeNet_Weights.DEFAULT')\n",
    "\n",
    "        # Change number of input channels\n",
    "        googlenet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.features = nn.Sequential(*list(googlenet.children())[:-1])\n",
    "        # Add a custom fully connected layer\n",
    "        self.fc = nn.Linear(googlenet.fc.in_features, num_classes)\n",
    "\n",
    "        # Initialize history dict\n",
    "        self.evaluation_metrics = evaluation_metrics\n",
    "        self.history = {key: [] for key in evaluation_metrics.keys()}\n",
    "        self.history_validation = {key: [] for key in evaluation_metrics.keys()}\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input has spatial dimensions (e.g., [batch_size, channels, height, width])\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        '''\n",
    "        Returns the predicted labels in evaluation mode: y_pred, y_true\n",
    "        '''\n",
    "        state = False if self.training is False else True\n",
    "        if state: self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for i, (x_minibatch, y_true_batch) in enumerate(dataloader):\n",
    "                y_pred_batch = F.softmax(self(x_minibatch), dim=1)\n",
    "                y_true.extend(y_true_batch.tolist())\n",
    "                y_pred.extend(y_pred_batch.tolist())\n",
    "        if state: self.train()\n",
    "        return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used for training: cpu\n",
      "Epoch [1/1]  loss: 1.2801, acc: 0.4635, macro recall: 0.2500\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Now define the metrics you want to monitor during training and save them in a dict (Important: All need to take y_pred, y_cls, y_true as input (This order!))\n",
    "def criterion_function(y_pred, y_cls, y_true):\n",
    "    return criterion(torch.tensor(y_pred), torch.tensor(y_true))\n",
    "def accuracy_function(y_pred, y_cls, y_true):\n",
    "    return accuracy_score(y_true, y_cls)\n",
    "def recall_function(y_pred, y_cls, y_true):\n",
    "    return recall_score(y_true, y_cls, average='macro')\n",
    "metrics = {'loss' : criterion_function, 'acc' : accuracy_function, 'macro recall' : recall_function}\n",
    "\n",
    "# Define Model\n",
    "model_googlenet = CustomGoogleNet(num_classes=4, evaluation_metrics=metrics)\n",
    "\n",
    "# Now define the loss (criterion), optimizer, lr_scheduler, \n",
    "early_stopper = EarlyStopper(model_googlenet, patience=5, min_delta=0)\n",
    "optimizer = torch.optim.Adam(model_googlenet.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_network(model_googlenet, train_loader, criterion, optimizer, 70, scheduler, validation_loader, device, early_stopper)\n",
    "\n",
    "torch.save(model_googlenet, 'model_googlenet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = model_googlenet.evaluate(test_loader)\n",
    "y_cls = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_true, y_cls))\n",
    "print(confusion_matrix(y_true, y_cls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewha_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
