{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from functions import *\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device.type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4\n",
      "Number of images: 6400\n",
      "\n",
      "Number of training images: 3840\n",
      "Number of validation images: 1280\n",
      "Number of testing images: 1280\n",
      "\n",
      "Number of training images: 3840\n",
      "Number of validation images: 1280\n",
      "Number of testing images: 1280\n",
      "\n",
      "Number of training batches: 60\n",
      "Number of validation batches: 20\n",
      "Number of testing batches: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from customdataset import *\n",
    "from augmentation import *\n",
    "\n",
    "train_per = 0.6 # 60% of the data is used for training\n",
    "val_per = 0.2 # 20% of the data is used for validation\n",
    "test_per = 0.2 # 20% of the data is used for testing\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "X, Y, num_classes = import_data()\n",
    "\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Number of images: {len(X)}')\n",
    "print(\"\")\n",
    "\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = split_data(X, Y, train_per=train_per, val_per=val_per, test_per=test_per)\n",
    "\n",
    "print(f'Number of training images: {len(X_train)}')\n",
    "print(f'Number of validation images: {len(X_val)}')\n",
    "print(f'Number of testing images: {len(X_test)}')\n",
    "print(\"\")\n",
    "\n",
    "dataset_train = MyDataset(X_train, Y_train, augment=True)\n",
    "dataset_val = MyDataset(X_val, Y_val, augment=False)\n",
    "dataset_test = MyDataset(X_test, Y_test, augment=False)\n",
    "\n",
    "print(f'Number of training images: {len(dataset_train)}')\n",
    "print(f'Number of validation images: {len(dataset_val)}')\n",
    "print(f'Number of testing images: {len(dataset_test)}')\n",
    "print(\"\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'Number of training batches: {len(train_loader)}')\n",
    "print(f'Number of validation batches: {len(val_loader)}')\n",
    "print(f'Number of testing batches: {len(test_loader)}')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data_loader \u001b[39m=\u001b[39m train_loader\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdataloader_to_numpy\u001b[39m(dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     images \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "data_loader = train_loader\n",
    "\n",
    "def dataloader_to_numpy(dataloader):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch_images, batch_labels = batch\n",
    "        images.append(batch_images.numpy())\n",
    "        labels.append(batch_labels.numpy())\n",
    "    \n",
    "    return np.concatenate(images), np.concatenate(labels)\n",
    "\n",
    "# Convert DataLoader to NumPy array\n",
    "X, Y = dataloader_to_numpy(data_loader)\n",
    "\n",
    "# Display 10 images\n",
    "plt.figure(figsize=(15, 7))\n",
    "num_images_to_display = 10\n",
    "for i in range(min(num_images_to_display, len(X))):\n",
    "    image = X[i].squeeze()  # Squeeze to remove the channel dimension for grayscale\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(image, cmap='gray')  # Specify the colormap for grayscale images\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, evaluation_metrics):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        # resnet = models.resnet18(pretrained=True)\n",
    "        resnet = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "        # Change number of input channels\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Add a custom fully connected layer\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "        # Initialize history dict\n",
    "        self.evaluation_metrics = evaluation_metrics\n",
    "        self.history = {key: [] for key in evaluation_metrics.keys()}\n",
    "        self.history_validation = {key: [] for key in evaluation_metrics.keys()}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        '''\n",
    "        Returns the predicted labels in evaluation mode: y_pred, y_true\n",
    "        '''\n",
    "        state = False if self.training is False else True\n",
    "        if state: self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for i, (x_minibatch, y_true_batch) in enumerate(dataloader):\n",
    "                y_pred_batch = F.softmax(self(x_minibatch), dim=1)\n",
    "                y_true.extend(y_true_batch.tolist())\n",
    "                y_pred.extend(y_pred_batch.tolist())\n",
    "        if state: self.train()\n",
    "        return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used for training: cpu\n",
      "Epoch [1/70]  loss: 1.3044, acc: 0.4172, macro recall: 0.3062\n",
      "Epoch [2/70]  loss: 1.2469, acc: 0.4688, macro recall: 0.2881\n",
      "Epoch [3/70]  loss: 1.4196, acc: 0.1695, macro recall: 0.2695\n",
      "Epoch [4/70]  loss: 1.1609, acc: 0.5672, macro recall: 0.3930\n",
      "Epoch [5/70]  loss: 1.3377, acc: 0.2828, macro recall: 0.2847\n",
      "Epoch [6/70]  loss: 1.1599, acc: 0.5406, macro recall: 0.3023\n",
      "Epoch [7/70]  loss: 1.1575, acc: 0.5547, macro recall: 0.3070\n",
      "Epoch [8/70]  loss: 1.1489, acc: 0.5500, macro recall: 0.3144\n",
      "Epoch [9/70]  loss: 1.0676, acc: 0.6727, macro recall: 0.4282\n",
      "Epoch [10/70]  loss: 1.0731, acc: 0.6687, macro recall: 0.5308\n",
      "Epoch [11/70]  loss: 0.9331, acc: 0.8422, macro recall: 0.8048\n",
      "Epoch [12/70]  loss: 0.9755, acc: 0.7742, macro recall: 0.5983\n",
      "Epoch [13/70]  loss: 1.3538, acc: 0.3570, macro recall: 0.5810\n",
      "Epoch [14/70]  loss: 0.9225, acc: 0.8258, macro recall: 0.6397\n",
      "Epoch [15/70]  loss: 0.9135, acc: 0.8375, macro recall: 0.8856\n",
      "Epoch [16/70]  loss: 0.8131, acc: 0.9359, macro recall: 0.9145\n",
      "Epoch [17/70]  loss: 0.7983, acc: 0.9492, macro recall: 0.9654\n",
      "Epoch [18/70]  loss: 0.8502, acc: 0.8992, macro recall: 0.9315\n",
      "Epoch [19/70]  loss: 0.8479, acc: 0.8992, macro recall: 0.8451\n",
      "Epoch [20/70]  loss: 0.8268, acc: 0.9195, macro recall: 0.9546\n",
      "Epoch [21/70]  loss: 0.7921, acc: 0.9508, macro recall: 0.9253\n",
      "Epoch [22/70]  loss: 0.7866, acc: 0.9563, macro recall: 0.9297\n",
      "Epoch [23/70]  loss: 0.7865, acc: 0.9594, macro recall: 0.9710\n",
      "Epoch [24/70]  loss: 0.7937, acc: 0.9500, macro recall: 0.9458\n",
      "Epoch [25/70]  loss: 0.7882, acc: 0.9578, macro recall: 0.9702\n",
      "Epoch [26/70]  loss: 0.7863, acc: 0.9570, macro recall: 0.9695\n",
      "Epoch [27/70]  loss: 0.7851, acc: 0.9617, macro recall: 0.9733\n",
      "Epoch [28/70]  loss: 0.7841, acc: 0.9625, macro recall: 0.9737\n",
      "Epoch [29/70]  loss: 0.7833, acc: 0.9625, macro recall: 0.9717\n",
      "Epoch [30/70]  loss: 0.7850, acc: 0.9594, macro recall: 0.9696\n",
      "Epoch [31/70]  loss: 0.7841, acc: 0.9594, macro recall: 0.9696\n",
      "Epoch [32/70]  loss: 0.7842, acc: 0.9578, macro recall: 0.9699\n",
      "Epoch [33/70]  loss: 0.7831, acc: 0.9602, macro recall: 0.9725\n",
      "Epoch [34/70]  loss: 0.7819, acc: 0.9633, macro recall: 0.9738\n",
      "Epoch [35/70]  loss: 0.7842, acc: 0.9594, macro recall: 0.9723\n",
      "Epoch [36/70]  loss: 0.7819, acc: 0.9617, macro recall: 0.9541\n",
      "Epoch [37/70]  loss: 0.7841, acc: 0.9586, macro recall: 0.9716\n",
      "Epoch [38/70]  loss: 0.7815, acc: 0.9633, macro recall: 0.9739\n",
      "Epoch [39/70]  loss: 0.7820, acc: 0.9625, macro recall: 0.9739\n",
      "Epoch [40/70]  loss: 0.7812, acc: 0.9641, macro recall: 0.9745\n",
      "Epoch [41/70]  loss: 0.7807, acc: 0.9648, macro recall: 0.9745\n",
      "Epoch [42/70]  loss: 0.7807, acc: 0.9656, macro recall: 0.9749\n",
      "Epoch [43/70]  loss: 0.7816, acc: 0.9625, macro recall: 0.9735\n",
      "Epoch [44/70]  loss: 0.7806, acc: 0.9617, macro recall: 0.9730\n",
      "Epoch [45/70]  loss: 0.7820, acc: 0.9602, macro recall: 0.9722\n",
      "Epoch [46/70]  loss: 0.7813, acc: 0.9633, macro recall: 0.9743\n",
      "Epoch [47/70]  loss: 0.7811, acc: 0.9633, macro recall: 0.9743\n",
      "Epoch [48/70]  loss: 0.7808, acc: 0.9633, macro recall: 0.9738\n",
      "Epoch [49/70]  loss: 0.7811, acc: 0.9625, macro recall: 0.9735\n",
      "Epoch [50/70]  loss: 0.7808, acc: 0.9617, macro recall: 0.9730\n",
      "Epoch [51/70]  loss: 0.7807, acc: 0.9617, macro recall: 0.9728\n",
      "Early stopping triggered and model parameters has been set to best validation parameters\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Now define the metrics you want to monitor during training and save them in a dict (Important: All need to take y_pred, y_cls, y_true as input (This order!))\n",
    "def criterion_function(y_pred, y_cls, y_true):\n",
    "    return criterion(torch.tensor(y_pred), torch.tensor(y_true))\n",
    "def accuracy_function(y_pred, y_cls, y_true):\n",
    "    return accuracy_score(y_true, y_cls)\n",
    "def recall_function(y_pred, y_cls, y_true):\n",
    "    return recall_score(y_true, y_cls, average='macro')\n",
    "metrics = {'loss' : criterion_function, 'acc' : accuracy_function, 'macro recall' : recall_function}\n",
    "\n",
    "# Define Model\n",
    "model18 = CustomResNet18(num_classes=4, evaluation_metrics=metrics)\n",
    "\n",
    "# Now define the loss (criterion), optimizer, lr_scheduler, \n",
    "early_stopper = EarlyStopper(model18, patience=7, min_delta=0)\n",
    "optimizer = torch.optim.Adam(model18.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_network(model18, train_loader, criterion, optimizer, 70, scheduler, val_loader, device, early_stopper)\n",
    "\n",
    "torch.save(model18, 'model_resnet_18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       640\n",
      "           1       0.97      0.95      0.96       448\n",
      "           2       0.94      0.94      0.94       179\n",
      "           3       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.96      1280\n",
      "   macro avg       0.97      0.97      0.97      1280\n",
      "weighted avg       0.96      0.96      0.96      1280\n",
      "\n",
      "[[624  11   5   0]\n",
      " [ 15 427   6   0]\n",
      " [  7   3 169   0]\n",
      " [  0   0   0  13]]\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = model18.evaluate(test_loader)\n",
    "y_cls = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_true, y_cls))\n",
    "print(confusion_matrix(y_true, y_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, evaluation_metrics):\n",
    "        super(CustomResNet50, self).__init__()\n",
    "        # resnet = models.resnet50(pretrained=True)\n",
    "        resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "        # Change number of input channels\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Add a custom fully connected layer\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "        # Initialize history dict\n",
    "        self.evaluation_metrics = evaluation_metrics\n",
    "        self.history = {key: [] for key in evaluation_metrics.keys()}\n",
    "        self.history_validation = {key: [] for key in evaluation_metrics.keys()}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        '''\n",
    "        Returns the predicted labels in evaluation mode: y_pred, y_true\n",
    "        '''\n",
    "        state = False if self.training is False else True\n",
    "        if state: self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for i, (x_minibatch, y_true_batch) in enumerate(dataloader):\n",
    "                y_pred_batch = F.softmax(self(x_minibatch), dim=1)\n",
    "                y_true.extend(y_true_batch.tolist())\n",
    "                y_pred.extend(y_pred_batch.tolist())\n",
    "        if state: self.train()\n",
    "        return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# criterion = nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# # Now define the metrics you want to monitor during training and save them in a dict (Important: All need to take y_pred, y_cls, y_true as input (This order!))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# optimizer = torch.optim.Adam(model50.parameters(), lr=0.01)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m train_network(model50, train_loader, criterion, optimizer, \u001b[39m70\u001b[39m, scheduler, val_loader, device, early_stopper)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/toby_linux/machine_learning/dementia-ml/transfer_learning.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model50, \u001b[39m'\u001b[39m\u001b[39mmodel_resnet_50.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_network' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Now define the metrics you want to monitor during training and save them in a dict (Important: All need to take y_pred, y_cls, y_true as input (This order!))\n",
    "def criterion_function(y_pred, y_cls, y_true):\n",
    "    return criterion(torch.tensor(y_pred), torch.tensor(y_true))\n",
    "def accuracy_function(y_pred, y_cls, y_true):\n",
    "    return accuracy_score(y_true, y_cls)\n",
    "def recall_function(y_pred, y_cls, y_true):\n",
    "    return recall_score(y_true, y_cls, average='macro')\n",
    "metrics = {'loss' : criterion_function, 'acc' : accuracy_function, 'macro recall' : recall_function}\n",
    "\n",
    "# Define Model\n",
    "model50 = CustomResNet50(num_classes=4, evaluation_metrics=metrics)\n",
    "\n",
    "# Now define the loss (criterion), optimizer, lr_scheduler, \n",
    "early_stopper = EarlyStopper(model50, patience=7, min_delta=0)\n",
    "optimizer = torch.optim.Adam(model50.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_network(model50, train_loader, criterion, optimizer, 70, scheduler, val_loader, device, early_stopper)\n",
    "\n",
    "torch.save(model50, 'model_resnet_50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model50 = torch.load('model_resnet_50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = model50.evaluate(test_loader)\n",
    "y_cls = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_true, y_cls))\n",
    "print(confusion_matrix(y_true, y_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGoogleNet(nn.Module):\n",
    "    def __init__(self, num_classes, evaluation_metrics):\n",
    "        super(CustomGoogleNet, self).__init__()\n",
    "        # googlenet = models.googlenet(pretrained=True)\n",
    "        googlenet = models.googlenet(weights='GoogLeNet_Weights.DEFAULT')\n",
    "\n",
    "        # Change number of input channels\n",
    "        googlenet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.features = nn.Sequential(*list(googlenet.children())[:-1])\n",
    "        # Add a custom fully connected layer\n",
    "        self.fc = nn.Linear(googlenet.fc.in_features, num_classes)\n",
    "\n",
    "        # Initialize history dict\n",
    "        self.evaluation_metrics = evaluation_metrics\n",
    "        self.history = {key: [] for key in evaluation_metrics.keys()}\n",
    "        self.history_validation = {key: [] for key in evaluation_metrics.keys()}\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input has spatial dimensions (e.g., [batch_size, channels, height, width])\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        '''\n",
    "        Returns the predicted labels in evaluation mode: y_pred, y_true\n",
    "        '''\n",
    "        state = False if self.training is False else True\n",
    "        if state: self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for i, (x_minibatch, y_true_batch) in enumerate(dataloader):\n",
    "                y_pred_batch = F.softmax(self(x_minibatch), dim=1)\n",
    "                y_true.extend(y_true_batch.tolist())\n",
    "                y_pred.extend(y_pred_batch.tolist())\n",
    "        if state: self.train()\n",
    "        return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used for training: cpu\n",
      "Epoch [1/70]  loss: 1.1790, acc: 0.5477, macro recall: 0.3060\n",
      "Epoch [2/70]  loss: 1.2586, acc: 0.4641, macro recall: 0.2983\n",
      "Epoch [3/70]  loss: 1.2705, acc: 0.4688, macro recall: 0.3059\n",
      "Epoch [4/70]  loss: 1.3585, acc: 0.3531, macro recall: 0.2516\n",
      "Epoch [5/70]  loss: 1.2374, acc: 0.5070, macro recall: 0.2554\n",
      "Epoch [6/70]  loss: 1.2318, acc: 0.4922, macro recall: 0.4218\n",
      "Epoch [7/70]  loss: 1.4450, acc: 0.1688, macro recall: 0.2660\n",
      "Epoch [8/70]  loss: 1.1216, acc: 0.6125, macro recall: 0.4226\n",
      "Epoch [9/70]  loss: 1.1895, acc: 0.5227, macro recall: 0.2702\n",
      "Epoch [10/70]  loss: 1.1176, acc: 0.6156, macro recall: 0.3545\n",
      "Epoch [11/70]  loss: 1.1583, acc: 0.5547, macro recall: 0.4951\n",
      "Epoch [12/70]  loss: 1.0550, acc: 0.6781, macro recall: 0.4421\n",
      "Epoch [13/70]  loss: 1.0764, acc: 0.6523, macro recall: 0.4132\n",
      "Epoch [14/70]  loss: 1.5131, acc: 0.2023, macro recall: 0.2938\n",
      "Epoch [15/70]  loss: 1.2275, acc: 0.5055, macro recall: 0.2541\n",
      "Epoch [16/70]  loss: 0.8885, acc: 0.8727, macro recall: 0.6287\n",
      "Epoch [17/70]  loss: 0.9224, acc: 0.8266, macro recall: 0.6000\n",
      "Epoch [18/70]  loss: 0.8620, acc: 0.8906, macro recall: 0.6462\n",
      "Epoch [19/70]  loss: 0.8645, acc: 0.8914, macro recall: 0.8435\n",
      "Epoch [20/70]  loss: 0.8764, acc: 0.8703, macro recall: 0.6758\n",
      "Epoch [21/70]  loss: 0.8537, acc: 0.8969, macro recall: 0.7825\n",
      "Epoch [22/70]  loss: 0.8204, acc: 0.9352, macro recall: 0.8899\n",
      "Epoch [23/70]  loss: 0.8371, acc: 0.9094, macro recall: 0.8067\n",
      "Epoch [24/70]  loss: 0.8123, acc: 0.9367, macro recall: 0.8379\n",
      "Epoch [25/70]  loss: 0.8290, acc: 0.9164, macro recall: 0.8080\n",
      "Epoch [26/70]  loss: 0.8061, acc: 0.9445, macro recall: 0.8785\n",
      "Epoch [27/70]  loss: 0.8078, acc: 0.9391, macro recall: 0.9344\n",
      "Epoch [28/70]  loss: 0.8299, acc: 0.9156, macro recall: 0.8005\n",
      "Epoch [29/70]  loss: 0.8155, acc: 0.9305, macro recall: 0.9136\n",
      "Epoch [30/70]  loss: 0.8074, acc: 0.9398, macro recall: 0.8738\n",
      "Epoch [31/70]  loss: 0.8048, acc: 0.9430, macro recall: 0.8755\n",
      "Epoch [32/70]  loss: 0.8049, acc: 0.9422, macro recall: 0.9002\n",
      "Epoch [33/70]  loss: 0.8044, acc: 0.9430, macro recall: 0.8408\n",
      "Epoch [34/70]  loss: 0.8018, acc: 0.9469, macro recall: 0.8601\n",
      "Epoch [35/70]  loss: 0.8043, acc: 0.9430, macro recall: 0.9212\n",
      "Epoch [36/70]  loss: 0.8013, acc: 0.9484, macro recall: 0.9176\n",
      "Epoch [37/70]  loss: 0.8004, acc: 0.9437, macro recall: 0.8994\n",
      "Epoch [38/70]  loss: 0.8015, acc: 0.9445, macro recall: 0.9113\n",
      "Epoch [39/70]  loss: 0.8015, acc: 0.9445, macro recall: 0.9170\n",
      "Epoch [40/70]  loss: 0.8009, acc: 0.9445, macro recall: 0.8822\n",
      "Epoch [41/70]  loss: 0.8013, acc: 0.9437, macro recall: 0.8616\n",
      "Epoch [42/70]  loss: 0.8012, acc: 0.9461, macro recall: 0.8833\n",
      "Epoch [43/70]  loss: 0.8005, acc: 0.9430, macro recall: 0.8599\n",
      "Epoch [44/70]  loss: 0.8005, acc: 0.9477, macro recall: 0.8832\n",
      "Early stopping triggered and model parameters has been set to best validation parameters\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Now define the metrics you want to monitor during training and save them in a dict (Important: All need to take y_pred, y_cls, y_true as input (This order!))\n",
    "def criterion_function(y_pred, y_cls, y_true):\n",
    "    return criterion(torch.tensor(y_pred), torch.tensor(y_true))\n",
    "def accuracy_function(y_pred, y_cls, y_true):\n",
    "    return accuracy_score(y_true, y_cls)\n",
    "def recall_function(y_pred, y_cls, y_true):\n",
    "    return recall_score(y_true, y_cls, average='macro')\n",
    "metrics = {'loss' : criterion_function, 'acc' : accuracy_function, 'macro recall' : recall_function}\n",
    "\n",
    "# Define Model\n",
    "model_googlenet = CustomGoogleNet(num_classes=4, evaluation_metrics=metrics)\n",
    "\n",
    "# Now define the loss (criterion), optimizer, lr_scheduler, \n",
    "early_stopper = EarlyStopper(model_googlenet, patience=7, min_delta=0)\n",
    "optimizer = torch.optim.Adam(model_googlenet.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_network(model_googlenet, train_loader, criterion, optimizer, 70, scheduler, val_loader, device, early_stopper)\n",
    "\n",
    "torch.save(model_googlenet, 'model_googlenet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       640\n",
      "           1       0.92      0.96      0.94       448\n",
      "           2       0.96      0.89      0.92       179\n",
      "           3       0.67      0.77      0.71        13\n",
      "\n",
      "    accuracy                           0.95      1280\n",
      "   macro avg       0.88      0.89      0.88      1280\n",
      "weighted avg       0.95      0.95      0.95      1280\n",
      "\n",
      "[[613  23   1   3]\n",
      " [ 15 429   4   0]\n",
      " [  4  14 159   2]\n",
      " [  1   1   1  10]]\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = model_googlenet.evaluate(test_loader)\n",
    "y_cls = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_true, y_cls))\n",
    "print(confusion_matrix(y_true, y_cls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewha_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
